
## 神经网络的输出单元

### 线性单元
一般来说，线性输出用于回归问题。对于最后一个隐层的输出h，线性单元输出y的估计$\ \hat{y} = W^Th + b$对于回归问题来说，y可视为因为模型特征不完备及噪声而具有一个高斯分布，而y的期望即是模型输出的估计值，即：
$$p(y|x) = N(y;\hat{y},I).$$此时最大化其对数似然及等价于最小化y与$\hat{y}$的均方误差。对于输出单元来说，输入与输出值相同，目标函数梯度与模型输出呈线性关系。故不会梯度饱和，使之易于采用梯度下降法优化。

### sigmoid单元
sigmoid单元一般用于二分类问题，也即y具有Bernoulli分布。模型输出可视为y取正例的概率：$p(y=1|x)$。也可视为y的期望。sigmoid的输出可表示为：$$\hat{y} = \sigma(w^Th + b)$$二分类问题一般用负对数似然来作为代价函数，因为代价函数中的log抵消了sigmoid函数里的exp。不然由于输出单元输入过大或过小造成sigmoid函数梯度饱和，从而难以基于梯度优化。如下，损失函数为：$$\begin{split}J(\theta) &= -log\ P(y|x)\\\&=-log\ \sigma((2y-1)z)\\\&=\zeta((1-2y)z)\end{split}$$我们可以把代价函数写成softplus函数的形式（z是输出单元的输入）。softplus的函数性质表明只有在自变量取较小的负值时梯度才会饱和，而(1-2y)z只有在输出极度正确的情况下才会得到一个较小的值。而在输出极度错误时，softplus的梯度不会收敛（趋近于1），这意味着基于梯度的学习可以很快改正错误。而类似于均方误差之类的代价函数则梯度会在sigmoid函数梯度饱和是饱和，哪怕输出极度错误。因为：$$\frac{\partial J}{\partial z} = \frac{\partial J}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial z}$$此时若z取得较大或较小值则等式右边第二项趋近于0,而等式右边第一项是随$\hat{y}$线性变化的，从而造成反向传播梯度过小，难以优化。
>注意：因为sigmoid函数可能因为输入过低下溢到0，而对0取对数为负无穷。故一般将代价函数表示为z的函数。

### softmax单元
可以把softmax单元视为sigmoid在多分类问题上的拓展。多分类问题y服从Multinoulli分布，而神经网络要做的是输出这个多项分布的参数估计，所以希望在输出单元对输入作归一化处理。softmax函数的形式为：$$softmax(z)_i = \frac{exp(z_i)}{\sum_j exp(z_j)}$$可以看到softmax在输入取指数后进行了归一化，取指数的好处自然也是对数似然中的log可以抵消exp。$$-log\ softmax(z)_i = -z_i + log\sum_j exp(z_j)$$可以看到以负对数似然为代价函数的话，鼓励第一项$z_i$取较大的值，而第二项鼓励压低所有z值。实际上第二项在z最大项相比其他z值较大时有如下性质：$$log\sum_j exp(z_j) \approx max_j z_j$$所以代价函数总是强烈的惩罚最大的不正确项。而当正确项最大时，第一项和第二项可以大致抵消，因此不会产生大的代价。代价函数的代价主要还是由分类不正确的样本产生的。
与sigmoid函数类似，softmax函数也会饱和。当某个输入值远大于其他输入时，其对应的输出项会饱和到1，其他1项会饱和到0。从神经科学的角度来看，这可以描述为“赢者通吃”的形式(即强势输出会抑制其他输出)。与sigmoid类似，如果不在代价函数中对饱和进行补偿的话，会造成代价函数对输入饱和从而难以学习。


----
以上内容总结于[Deep learning chapter6](http://www.deeplearningbook.org/contents/mlp.html).